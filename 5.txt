Design document:
steps to crawl the data
1) If we have to write a general-purpose crawler to download different media types, we might have to break down the parsing module into different sets of modules:
one for HTML
another for images
another for videos

2)Here each module extracts what is considered interesting for that media type.

3) Let’s assume for now that our crawler is going to deal with HTML only, but it should be extensible and make it easy to add support for new media types.

4)Algorithm 

The basic algorithm executed by any Web crawler is to take a list of seed URLs as its input.
And then repeatedly execute the following steps:
Pick a URL from the unvisited URL list.
Determine the IP Address of its host-name.
Establishing a connection to the host to download the corresponding document.
Parse the document contents to look for new URLs.
Add the new URLs to the list of unvisited URLs.
Process the downloaded document, e.g., store it or index its contents, etc.
Go back to step-1.

5)Detail Design 

Let’s assume our crawler is running on one server, and all the crawling is done by multiple working threads.

Each working thread performs all the steps needed to download and process the document in a loop.

The first step of this loop is to remove an absolute URL from the shared URL frontier for downloading.

An absolute URL begins with a scheme (e.g., “HTTP”), which identifies the network protocol that should be used to download it.

We can implement these protocols in a modular way for extensibility, so that later our crawler can support more protocols.

Based on the URL’s scheme, the worker calls the appropriate protocol module to download the document.
After downloading, the document is placed into a Document Input Stream (DIS).

Putting documents into DIS will enable other modules to re-read the document multiple times.

Once the document has been written to DIS, the worker thread invokes the dedupe test to determine whether this document (associated with a different URL) has been seen before.

If so, the document is not processed any further, and the worker thread removes the next URL from the frontier.

Next, our crawler needs to process the downloaded document.

Each document can have a different MIME type like HTML page, Image, Video, etc.

We can implement these MIME schemes in a modular way so that later our crawler can support more types.

Based on MIME type of document, worker invokes the process method of each processing module associated with that MIME type.

Furthermore, our HTML processing module will extract all links from the page.

Each link is converted into an absolute URL and tested against a user-supplied URL filter to determine if it should be downloaded.

If the URL passes the filter, the worker performs the URL-seen test, which checks if the URL has been seen before, namely, if it is in the URL frontier or has already been downloaded.

If the URL is new, it is added to the frontier.
